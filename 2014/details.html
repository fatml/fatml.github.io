
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>FAT ML: Fairness Accountability and Transparency in Machine
Learning</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Custom styles for this template -->
    <link href="css/carousel.css" rel="stylesheet">
  </head>
<!-- NAVBAR
================================================== -->
  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">FAT ML 2014</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html#scope">Scope</a></li>
            <li><a href="speakers.html">Speakers</a></li>
            <li><a href="details.html#location">Venue</a></li>
            <li><a href="details.html#schedule">Schedule</a></li>
            <li><a href="details.html#abstracts">Abstracts</a></li>
            <li><a href="resources.html">Resources</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li><a href="speakers.html#organizers">Contact Us</a></li>
        	<li><a href="index.html#sponsor">Sponsor</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

<a id="date">
</a>
<a id="location">
</a>

<h1 class="featurette-heading extra-space">Date and Location</h1>

	<div class="container" style="font-size:14pt">
        <p><strong>Date:</strong> Friday, December 12, 2014</p>
        <p><strong>Location:</strong> Co-located with <a
                href="http://nips.cc/Conferences/2014/">NIPS 2014</a>, Montreal, Canada</strong></p>
<p><strong>How to attend?</strong> The workshop is open to everyone.
However, you will need to register for the NIPS 2014 workshop program. 
Instructions are available on the <a href="https://nips.cc/">NIPS web site</a>. There is a central workshop
registration fee that gives you access to all workshops including ours.</p>
	</div>

<a id="schedule">
</a>
<h1 class="featurette-heading extra-space">Schedule</h1>


	<div class="container" style="font-size:14pt">
      <div class="row">
        <div class="col-lg-12" style="text-align:center;background-color:#eee;padding:15px;">
		Morning Session (9:00AM &ndash; 12:45PM)
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		9:00
		</div>
        <div class="col-lg-4">
		Solon Barocas, Moritz Hardt
		</div>
        <div class="col-lg-7">
            <a href="#solon">Introduction</a>
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		9:20
		</div>
        <div class="col-lg-4">
		Cynthia Dwork
		</div>
        <div class="col-lg-7">
            <a href="#cynthia">Can We Learn to Be Fair?</a>
		</div>
	  </div>


      <div class="row" style="background-color:#aba;">
        <div class="col-lg-1">
		10:00
		</div>
        <div class="col-lg-11">
		Coffee Break (30min)
		</div>
	  </div>

     <div class="row">
        <div class="col-lg-1">
		10:30
		</div>
        <div class="col-lg-4">
            Rich Zemel
		</div>
        <div class="col-lg-7">
            <a href="#rich">Learning Rich But Fair Representations</a>
		</div>
	  </div>


      <div class="row">
        <div class="col-lg-1">
		11:10
		</div>
        <div class="col-lg-4">
            Hanna Wallach
		</div>
        <div class="col-lg-7">
            <a href="#hanna">Moving Beyond Prediction: Big Data, Transparency,
                and Accountability</a>
		</div>
	  </div>

      
      <div class="row" style="background-color:#aba;">
        <div class="col-lg-1">
		11:50
		</div>
        <div class="col-lg-11">
		Short Break (10min)
		</div>
	  </div>


      <div class="row">
        <div class="col-lg-1">
		12:00
		</div>
        <div class="col-lg-4">
            Anupam Datta, Michael Tschantz
		</div>
        <div class="col-lg-7">
            <a href="#anupam">Privacy through Accountability: Information Flow
                Experiments</a>
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		12:40
		</div>
        <div class="col-lg-4">
            Sorelle Friedler, Carlos Scheidegger,  Suresh Venkatasubramanian
		</div>
        <div class="col-lg-7">
            <a href="#friedler">Certifying and Removing Disparate Impact</a>
		</div>
	  </div>


      <div class="row" style="background-color:#aba;">
        <div class="col-lg-1">
            12:55 
		</div>
        <div class="col-lg-11">
            Lunch Break (2h5min)
		</div>
    </div>

      <div class="row">
        <div class="col-lg-12" style="text-align:center;background-color:#eee;padding:15px;">
		Afternoon Session (3:00PM &ndash; 6:30PM)
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		3:00
		</div>
        <div class="col-lg-4">
         Ed Felten, Josh Kroll
		</div>
        <div class="col-lg-7">
    <a href="#kroll">Accountable Algorithms</a>
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		3:40
		</div>
        <div class="col-lg-4">
            David Robinson, Harlan Yu
		</div>
        <div class="col-lg-7">
		<a href="#robinson">Civil Rights and Machine Learning: Emerging Policy Questions</a>
		</div>
	  </div>

	<div class="row" style="background-color:#aba;">
        <div class="col-lg-1">
		4:30
		</div>
        <div class="col-lg-11">
		Coffee Break  (30min)
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		5:00
		</div>
        <div class="col-lg-4">
            Rayid Ghani, Cathy O'Neil, Foster Provost            
		</div>
        <div class="col-lg-7">
        	<a href="#panel">A Closing Panel Discussion</a>
		</div>
	  </div>

	</div>

<a id="abstracts">
</a>

<h1 class="featurette-heading extra-space">Presentations</h1>

<div class="container" style="font-size:14pt">
    <h2><a name="solon"></a>Solon Barocas and Moritz Hardt</h2>
    <h3>Introduction</h3>
    
        <br>
        
<center><a class="btn btn-default" href="http://www.fatml.org/2014/presentations/barocas.pdf" role="button">Slides</a> <a class="btn btn-default" href="http://www.fatml.org/2014/recordings/01%20-%20Solon%20Barocas%20and%20Moritz%20Hardt%20-%20Introduction.mp3" role="button">Audio Recording</a></center>


<div class="container" style="font-size:14pt">
    <h2><a name="cynthia"></a>Cynthia Dwork</h2>
    <h3>Can we Learn to be Fair?</h3>

    <br>
    
<center><a class="btn btn-default" href="http://www.fatml.org/2014/recordings/02%20-%20Cynthia%20Dwork%20-%20Can%20We%20Learn%20to%20Be%20Fair.mp3" role="button">Audio Recording</a></center>

    <br>
    

    <p>
&quot;Fairness Through Awareness&quot; is an approach to fairness in
classification, where the goal is to prevent discrimination against protected
population subgroups in classification systems while simultaneously preserving
utility for the party carrying out the classification (eg, an advertiser,
bank, or admissions committee).  We argue that a classification is fair only
when individuals who are similar with respect to the classification task at
hand are treated similarly, and this in turn requires understanding of
sub-cultures of the population.  In consequence, hiding information from a
classifying algorithm can result in less fairness (and less utility):
&quot;privacy&quot; does not yield fairness.</p>

    <p>
We obtain a computational solution that, given a similarity metric defining, for each pair of individuals, their similarity with respect to the given classification task, achieves our fairness goals.  The metric should represent ground truth, but how can it be obtained?  Can learning help?
    </p>

    <p>
We also discuss the crescendo of calls for &quot;comprehensible&quot; or
&quot;interpretable&quot; classifiers that &quot;explain&quot; individual
classifications, and suggest a new desideratum, which we call
&quot;negotiability,&quot; as a direction for future research.
    </p>

    <p>
Joint work with Hardt, Pitassi, Reingold, and Zemel.
    </p>
    
</p>
</div>

<div class="container" style="font-size:14pt">
    <h2><a name="hanna"></a>Hanna Wallach</h2>
    <h3>Moving Beyond Prediction: Big Data, Transparency, and Accountability</h3>
    <br>
    
<center><a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Slides</a> <a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Audio Recording</a></center>

    <br>

    <p>
This talk will be structured around four talking points -- intended to
prompt discussion -- that lie at the heart of fairness and
transparency in machine learning: data, questions, models, and
findings. In discussing these talking points, I will touch upon
limitations of the most prevalent definitions of big data; the need
for true collaborations with social scientists when analyzing social
data; data-driven research vs. question-driven research; convenience
data vs. carefully selected data; transparency and algorithmic
accountability reporting; models for exploration/explanation
vs. models for prediction; representing and maintaining uncertainty;
error analysis; intuition and bias in interpreting findings; and,
finally, the importance of scientific communication.
    </p>
</div>


<div class="container" style="font-size:14pt">
	<h2><a name="rich"></a>Rich Zemel</h2>
    <h3>Learning Rich But Fair Representations</h3>
    
        <br>
    
<center><a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Slides</a> <a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Audio Recording</a></center>

    <br>
    
    <p>
We propose a learning algorithm for fair classification that
achieves both group fairness (the proportion of members in a
protected group receiving positive classification is identical to
the proportion in the population as a whole), and individual
fairness (similar individuals should be treated similarly).  We
formulate fairness as an optimization problem of finding a good
representation of the data with two competing goals: to encode the
data as well as possible, while simultaneously obfuscating any
information about membership in the protected group.  I will
present some alternative formulations for the two main components
of this framework, the measure of fairness, and the form of
the learned representation. I will show empirical comparisons
of these with earlier formulations on two datasets.
</p>
</div>
    
    
    	<div class="container" style="font-size:14pt">
	<h2><a name="friedler"></a>Sorelle Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian</h2>
    <h3>Certifying and Removing Disparate Impact</h3>

    <br>
    
<center><a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Slides</a> <a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Audio Recording</a></center>

    <br>

<p>What does it mean for an algorithm to be biased?</p>
<p>In U.S. law, the notion of bias is typically encoded through the idea of disparate impact: namely, that a process (hiring, selection, etc) that on the surface seems completely neutral might still have widely different impacts on different groups. This legal determination expects an explicit understanding of the selection process.</p>
<p>If the process is an algorithm though (as is common these days), the process of determining disparate impact (and hence bias) becomes trickier. Firstly, it might not be possible to disclose the process. Secondly, even if the process is open, it might be too complex to ascertain how the algorithm is making its decisions. In effect, since we don’t have access to the algorithm, we must make inferences based on the data it uses.</p>
<p>We make three contributions to this problem. Firstly, we link the legal notion of disparate impact to a measure of classification accuracy that while known, has not received as much attention as more traditional notions of accuracy. Secondly, we propose a test for the possibility of bias based on analyzing the information leakage of protected information from the data. Finally, we describe methods by which data might be made “unbiased” in order to test an algorithm. Interestingly, our approach bears some resemblance to actual practices that have recently received legal scrutiny.</p>
	</div>

	<div class="container" style="font-size:14pt">
	<h2><a name="anupam"></a>Anupam Datta</h2>
    <h3>Privacy through Accountability: Information Flow Experiments</h3>
    
        <br>
    
<center><a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Slides</a> <a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Audio Recording</a></center>

    <br>

<p>Privacy through accountability refers to the principle that entities that
hold personal information about individuals are accountable for adopting
measures that protect the privacy of the data subjects. Computational
approaches to privacy through accountability involve developing algorithms and
tools that can be used to provide internal and external oversight about the
practices of such entities. After providing an overview of this emerging
research area, I will focus on one of our recent results in Web privacy.</p>

<p>I will describe the problem of detecting personal data usage by websites
when the analyst does not have access to the code of the system nor full
control over the inputs or observability of all outputs of the system. A
concrete example of this setting is one in which a privacy advocacy group, a
government regulator, or a Web user may be interested in checking whether a
particular web site uses certain types of personal information for
advertising. I will present a methodology for <strong>information flow
experiments</strong>
based on experimental science and statistical analysis that addresses this
problem, our tool AdFisher that incorporates this methodology, and findings of
<strong>opacity</strong>, <strong>choice</strong> and
<strong>discrimination</strong> from our experiments with Google. These
results also raise interesting challenges for the design of new classes of
machine learning algorithms that provide transparency, respect choice, and are
non-discriminatory.</p>
	</div>

    <div class="container" style="font-size:14pt">
	<h2><a name="kroll"></a>Joshua Kroll and Ed Felten</h2>
    <h3>Accountable Algorithms</h3>

    <br>
    
<center><a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Slides</a> <a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Audio Recording</a></center>

    <br>

    <p>Important decisions about people are increasingly made by algorithms: Votes are counted; voter rolls are purged; financial aid decisions are made; taxpayers are chosen for audits; air travelers are selected for search; credit eligibility decisions are made.  Citizens, and society as a whole, have an interest in making these processes more transparent.  Yet the full basis for these decisions is rarely available to affected people: the algorithm or some inputs may be secret; or the implementation may be secret; or the process may not be precisely described. A person who suspects the process went wrong has little recourse. And an oversight authority who wants to ensure that decisions are made according to an acceptable policy has little assurance that proffered decision rules match decisions for actual users.</p>

    <p>To address this problem, we propose to use accountable algorithms, which provide both an result and a proof that can convince a skeptical party that a consistent policy was applied correctly to accurate data to produce the announced result. Critically, the proof can convince an observer while maintaining the secrecy of parts of the policy used to determine the output, and the privacy of individuals' personal data.</p> 

    <p>Our methods use the tools of computer science to cryptographycially ensure the technical properties that can be proven, while providing the necessary information so that a political, legal, or social oversight process can operate effectively.  Combining the technology of verified computation with the operation of non-technical governance structures offers the best hope of governing the operation of algorithmic decision processes in practice.</p>
   	</div>

    <div class="container" style="font-size:14pt">
	<h2><a name="robinson"></a>David Robinson and Harlan Yu</h2>
    <h3>Civil Rights and Machine Learning: Emerging Policy Questions</h3>

    <br>
    
<center><a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Slides</a> <a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Audio Recording</a></center>

    <br>

    <p>The key decisions that shape people’s lives—decisions about jobs, healthcare, housing, education, criminal justice and other key areas—are, more and more often, being made by machine learning systems. As a result, a growing number of important conversations about civil rights, which focus on how these decisions are made, are also becoming discussions about machine learning. Policymakers and the public increasingly want to understand how machine learning systems work in general, how they reach particular decisions, and (in some cases) how their operation might be altered to advance social goals.</p>

    <p>Some political requirements for automated decisions -- such as a requirement that decisions be reached using a consistent procedure, or be intelligible to human observers -- may lend themselves to technical solutions. But in the area of non-discrimination, U.S. law often avoids bright line rules, proceeding instead on a holistic, case-by-case basis. The kind of rules that engineers may need, in other words, may not exist today. Those rules may need to be developed anew, in a policy discussion that is informed by a deeper understanding of technical methods. This is both a challenge and an opportunity for the field of machine learning to help shape the future of civil rights.</p>
   	</div>

    <div class="container" style="font-size:14pt">
	<h2><a name="panel"></a>Rayid Ghani, Cathy O'Neil, and Foster Provost</h2>
    <h3>A Closing Panel Discussion</h3>

    <br>
    
<center><a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Slides</a> <a class="btn btn-default" href="http://www.haverford.edu/computerscience/faculty/sorelle/" role="button">Audio Recording</a></center>



    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
  </body>
</html>

