
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>FAT ML: Fairness Accountability and Transparency in Machine
Learning</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Custom styles for this template -->
    <link href="css/carousel.css" rel="stylesheet">
  </head>
<!-- NAVBAR
================================================== -->
  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">FAT ML 2015</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html#scope">Scope</a></li>
            <li><a href="speakers.html">Speakers</a></li>
            <li><a href="details.html#location">Venue</a></li>
            <li><a href="details.html#schedule">Schedule</a></li>
            <li><a href="details.html#abstracts">Abstracts</a></li>
			<li><a href="cfp.html">Papers</a></li>
            <li><a href="resources.html">Resources</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li><a href="speakers.html#organizers">Contact Us</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

<a id="date">
</a>
<a id="location">
</a>

<h1 class="featurette-heading extra-space">Date and Location</h1>

	<div class="container" style="font-size:14pt">
        <p><strong>Date:</strong> Satuday, July 11, 2015</p>
        <p><strong>Location:</strong> Co-located with <a
                href="http://icml.cc/2015/">ICML 2015</a>, Lille, France</strong></p>
<p><strong>How to attend?</strong> The workshop is open to everyone.
However, you will need to register for the ICML 2015 workshop program. 
Instructions are available on the <a href="http://icml.cc/2015/">ICML web site</a>. There is a central workshop registration fee that gives you access to all workshops including ours.</p>
	</div>

<a id="schedule">
</a>
<h1 class="featurette-heading extra-space">Schedule</h1>

	<div class="container" style="font-size:14pt">
      <div class="row">
        <div class="col-lg-12" style="text-align:center;background-color:#eee;padding:15px;">
		Morning Session (9:00AM &ndash; 12:00PM)
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		9:00
		</div>
        <div class="col-lg-4">

		</div>
        <div class="col-lg-7">
		Introduction
		</div>
	  </div>

	        <div class="row">
        <div class="col-lg-1">
		9:10
		</div>
        <div class="col-lg-4">
		Nick Diakopoulos
		</div>
        <div class="col-lg-7">
            <a href="#diakopoulos">Algorithmic Accountability and Transparency in Journalism</a>
		</div>
	  </div>

      <div class="row" style="background-color:#aba;">
        <div class="col-lg-1">
		10:00
		</div>
        <div class="col-lg-11">
		Coffee Break (30min)
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		10:30
		</div>
        <div class="col-lg-4">
		Sara Hajian
		</div>
        <div class="col-lg-7">
            <a href="#hajian">Discrimination- and Privacy-Aware Data Mining</a>
		</div>
	  </div>

     <div class="row">
        <div class="col-lg-1">
		11:10
		</div>
        <div class="col-lg-4">
            Salvatore Ruggieri
		</div>
        <div class="col-lg-7">
            <a href="#ruggieri">Privacy Attacks and Anonymization Methods as Tools for Discrimination Discovery and Fairness</a>
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		11:50
		</div>
        <div class="col-lg-4">
            Toshihiro Kamishima and Kazuto Fukuchi
		</div>
        <div class="col-lg-7">
            <a href="#kamishima">Future Directions of Fairness-Aware Data Mining: Recommendation, Causality, and Theoretical Aspects</a>
		</div>
	  </div>
	
      <div class="row" style="background-color:#aba;">
        <div class="col-lg-1">
            12:30
		</div>
        <div class="col-lg-11">
            Lunch Break (1h30min)
		</div>
    </div>

      <div class="row">
        <div class="col-lg-12" style="text-align:center;background-color:#eee;padding:15px;">
		Afternoon Session (2:00PM &ndash; 6:00PM)
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		2:00
		</div>
        <div class="col-lg-4">
         	Muhammad Bilal Zafar, Isabel Valera Martinez, Manuel Gomez Rodriguez, and Krishna Gummadi
		</div>
        <div class="col-lg-7">
    <a href="#zafar">Fairness Constraints: A Mechanism for Fair Classification</a>
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		2:30
		</div>
        <div class="col-lg-4">
            Benjamin Fish, Jeremy Kun, and Ádám D. Lelkes
		</div>
        <div class="col-lg-7">
		<a href="#fish">Fair Boosting: A Case Study</a>
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		3:00
		</div>
        <div class="col-lg-4">
            Zubin Jelveh and Michael Luca
		</div>
        <div class="col-lg-7">
		<a href="#jelveh">Towards Diagnosing Accuracy Loss in Discrimination-Aware Classification: An Application to Predictive Policing</a>
		</div>
	  </div>
	  
	  <div class="row">
        <div class="col-lg-1">
		3:30
		</div>
        <div class="col-lg-4">
            Indrė Žliobaitė
		</div>
        <div class="col-lg-7">
		<a href="#zliobaite">On the Relation between Accuracy and Fairness in Binary Classification</a>
		</div>
	  </div>
	  
	<div class="row" style="background-color:#aba;">
        <div class="col-lg-1">
		4:00
		</div>
        <div class="col-lg-11">
		Coffee Break  (30min)
		</div>
	  </div>

      <div class="row">
        <div class="col-lg-1">
		4:30
		</div>
        <div class="col-lg-4">
            Fernando Diaz, Sorelle Friedler, Mykola Pechenizkiy, Hanna Wallach, and
            Suresh Venkatasubramanian (Moderator)
		</div>
        <div class="col-lg-7">
            A Closing Panel Discussion
		</div>
	  </div>

	</div>

<a id="abstracts">
</a>

<h1 class="featurette-heading extra-space">Abstracts</h1>

<div class="container" style="font-size:14pt">
    <h2><a name="diakopoulos"></a>Nick Diakopoulos</h2>
    <h3>Algorithmic Accountability and Transparency in Journalism</h3>

    <p>A new form of computational journalism that I call "Algorithmic Accountability Reporting" is emerging to apply the core journalistic functions of watchdogging and investigative accountability reporting to algorithms. In this talk I will discuss how algorithmic accountability reporting is used by journalists as a mechanism for articulating the power structures, biases, and influences that computational artifacts play in society. And I’ll trace various legal, technical, and regulatory challenges that remain, offering new openings for the development of tools. Finally, I will discuss the ethical mandate for transparency of algorithms in use by both corporations and governments and proffer for discussion an initial transparency policy that delineates what dimensions of algorithms might be disclosed, either voluntarily or compulsory through future regulation.</p>

</div>

<div class="container" style="font-size:14pt">
    <h2><a name="hajian"></a>Sara Hajian</h2>
    <h3>Discrimination- and Privacy-Aware Data Mining</h3>

    <p>In the information society, massive and automated data collection occurs as a consequence of the ubiquitous digital traces we all generate in our daily life. The availability of such wealth of data makes its publication and analysis highly desirable for a variety of purposes, including policy making, planning, marketing, research, etc. Yet, the real and obvious benefits of data analysis and publishing have a dual, darker side. There are at least two potential threats for individuals whose information is published: privacy invasion and potential discrimination. Privacy invasion occurs when the values of published sensitive attributes can be linked to specific individuals (or companies). Discrimination is unfair or unequal treatment of people based on membership to a category, group or minority, without regard to individual characteristics.</p>

    <p>On the legal side, parallel to the development of privacy legislation, anti-discrimination legislation has undergone a remarkable expansion, and it now prohibits discrimination against protected groups on the grounds of race, color, religion, nationality, sex, marital status, age and pregnancy, and in a number of settings, like credit and insurance, personnel selection and wages, and access to public services. On the technology side, efforts at guaranteeing privacy have led to developing privacy preserving data mining (PPDM) and efforts at fighting discrimination have led to developing anti-discrimination techniques in data mining. Some proposals are oriented to the discovery and measurement of discrimination, while others deal with preventing data mining (DPDM) from becoming itself a source of discrimination, due to automated decision making based on discriminatory models extracted from inherently biased datasets. I will describe some of these techniques for discrimination prevention, simultaneous discrimination and privacy protection, and discrimination discovery and show some recent results.</p>

</div>

<div class="container" style="font-size:14pt">
    <h2><a name="ruggieri"></a>Salvatore Ruggieri</h2>
    <h3>Privacy Attacks and Anonymization Methods as Tools for Discrimination Discovery and Fairness</h3>

    <p>Social discrimination discovery from data aims to identify illegal and unethical discriminatory patterns towards protected-by-law groups. Fairness in machine learning aims at preventing the usage of such patterns in classifiers trained from data possibly containing them. In this talk, we introduce an intriguing parallel between the role of the anti-discrimination authority and the role of an attacker in private data publishing. The parallel leads to two approaches in re-using tools from the privacy research. On the one side, we deploy privacy attack strategies, such as Frëchet bounds attacks, as tools for indirect discrimination discovery. On the other side, we investigate the relation between attribute inference control methods and social discrimination models, showing that t closeness implies bd(t)-protection for a bound function bd(). This allows us to adapt data anonymization algorithms, such as Mondrian multidimensional generalization and Sabre bucketization and redistribution, to the purpose of non-discrimination data protection—a form of pre-processing that removes discriminatory patterns from training data.</p>

</div>

<div class="container" style="font-size:14pt">
    <h2><a name="kamishima"></a>Toshihiro Kamishima and Kazuto Fukuchi</h2>
    <h3>Future Directions of Fairness-Aware Data Mining: Recommendation, Causality, and Theoretical Aspects</h3>

    <p><p>The goal of fairness-aware data mining (FADM) is to analyze data while taking into account potential issues of fairness.   In this talk, we will cover three topics in FADM:</p>
<ol>
<li>Fairness in a Recommendation Context: In classification tasks, the term "fairness" is regarded as anti-discrimination.   We will present other types of problems related to the fairness in a recommendation context.</li>
<li>What is Fairness: Most formal definitions of fairness have a connection with the notion of statistical independence.   We will explore other types of formal fairness based on causality, agreement, and unfairness.</li>
<li>Theoretical Problems of FADM: After reviewing technical and theoretical open problems in the FADM literature, we will introduce the theory of the generalization bound in terms of accuracy as well as fairness.</li>
</ol>
<p>Joint work with <a href="http://www.mdl.cs.tsukuba.ac.jp/~jun/">Jun Sakuma</a>, <a href="https://staff.aist.go.jp/s.akaho/">Shotaro Akaho</a>, and <a href="https://staff.aist.go.jp/h.asoh/">Hideki Asoh</a></p></p>

</div>
	  
<div class="container" style="font-size:14pt">
    <h2><a name="zafar"></a>Muhammad Bilal Zafar, Isabel Valera Martinez, Manuel Gomez Rodriguez, and Krishna Gummadi</h2>
    <h3>Fairness Constraints: A Mechanism for Fair Classification</h3>

    <p>Automated data-driven decision systems are ubiquitous across a wide variety of online services, from online social networking and ecommerce to e-government. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead to user discrimination, even in the absence of intent.</p>

    <p>In this paper, we introduce fairness constraints, a mechanism to ensure fairness in a wide variety of classifiers in a principled manner. Fairness prevents a classifier from outputting predictions correlated with certain sensitive attributes in the data. We then instantiate fairness constraints on three well-known classifiers—logistic regression, hinge loss and support vector machines (SVM)—and evaluate their performance in a real-world dataset with meaningful sensitive human attributes. Experiments show that fairness constraints allow for an optimal trade-off between accuracy and fairness.</p>
    
</div>	  

<div class="container" style="font-size:14pt">
    <h2><a name="fish"></a>Benjamin Fish, Jeremy Kun, and Ádám D. Lelkes</h2>
    <h3>Fair Boosting: A Case Study</h3>

    <p>We study the classical AdaBoost algorithm in the context of fairness. We use the Census Income Dataset (Lichman, 2013) as a case study. We empirically evaluate the bias and error of four variants of AdaBoost relative to an unmodified AdaBoost baseline, and study the trade-offs between reducing bias and maintaining low error. We further define a new notion of fairness and measure it for all of our methods. Our proposed method, modifying the hypothesis output by AdaBoost by shifting the decision boundary for the protected group, outperforms the state of the art for the census dataset.</p>
    
</div>	  

<div class="container" style="font-size:14pt">
    <h2><a name="jelveh"></a>Zubin Jelveh and Michael Luca</h2>
    <h3>Towards Diagnosing Accuracy Loss in Discrimination-Aware Classification: An Application to Predictive Policing</h3>

    <p>Prediction algorithms are increasingly used to forecast outcomes of processes that are societally sensitive. In response, algorithms have been developed to produce fair classifications but at the potential cost of accuracy. In this work, we present a framework for modeling the pathways by which sensitive variables influence—and are influenced by—nonsensitive variables. These pathways allow us to discern between two types of accuracy loss: justified reduction due to underlying discrimination in the data, and overadjustment due to the removal of nonsensitive predictive information. We also present a framework for adjusting input data to remove the association between sensitive and nonsensitive predictors and assess its ability to produce fair classifications. Finally, we apply our methodology to a new dataset in the criminal justice domain.</p>
    
</div>	  
	  
<div class="container" style="font-size:14pt">
    <h2><a name="zliobaite"></a>Indrė Žliobaitė</h2>
    <h3>On the Relation between Accuracy and Fairness in Binary Classification</h3>

    <p>Our study revisits the problem of accuracy/fairness tradeoff in binary classification. We argue that comparison of non-discriminatory classifiers needs to account for different rates of positive predictions, otherwise conclusions about performance may be misleading, because accuracy and discrimination of naive baselines on the same dataset vary with different rates of positive predictions. We provide methodological recommendations for sound comparison of nondiscriminatory classifiers, and present a brief theoretical and empirical analysis of tradeoffs between accuracy and non-discrimination.</p>
    
</div>	 


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
  </body>
</html>

