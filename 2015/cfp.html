
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>FAT ML: Fairness Accountability and Transparency in Machine
Learning</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Custom styles for this template -->
    <link href="css/carousel.css" rel="stylesheet">
  </head>
<!-- NAVBAR
================================================== -->
  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">FAT ML 2015</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html#scope">Scope</a></li>
            <li><a href="speakers.html">Speakers</a></li>
            <li><a href="details.html#location">Venue</a></li>
            <li><a href="details.html#schedule">Schedule</a></li>
            <li><a href="details.html#abstracts">Abstracts</a></li>
			<li><a href="cfp.html">Papers</a></li>
            <li><a href="resources.html">Resources</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
        	<li><a href="https://lists.princeton.edu/cgi-bin/wa?SUBED1=fatml&A=1">Mailing List</a></li>
            <li><a href="speakers.html#organizers">Contact Us</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

<a id="date">
</a>
<a id="location">
</a>

<h1 class="featurette-heading extra-space">Papers</h1>

<div class="container" style="font-size:14pt">
    <h2>Muhammad Bilal Zafar, Isabel Valera Martinez, Manuel Gomez Rodriguez, and Krishna Gummadi</h2>
    <h3><a href="papers/Zafar_Valera_Gomez-Rodriguez_Gummadi.pdf">Fairness Constraints: A Mechanism for Fair Classification</a></h3>
  <p>Automated data-driven decision systems are ubiquitous across a wide variety of online services, from online social networking and ecommerce to e-government. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead to user discrimination, even in the absence of intent.</p>

    <p>In this paper, we introduce fairness constraints, a mechanism to ensure fairness in a wide variety of classifiers in a principled manner. Fairness prevents a classifier from outputting predictions correlated with certain sensitive attributes in the data. We then instantiate fairness constraints on three well-known classifiers—logistic regression, hinge loss and support vector machines (SVM)—and evaluate their performance in a real-world dataset with meaningful sensitive human attributes. Experiments show that fairness constraints allow for an optimal trade-off between accuracy and fairness.</p>
    
</div>	  

<div class="container" style="font-size:14pt">
    <h2><a name="fish"></a>Benjamin Fish, Jeremy Kun, and Ádám D. Lelkes</h2>
	<h3><a href="papers/Fish_Kun_Lelkes.pdf">Fair Boosting: A Case Study</a></h3>

    <p>We study the classical AdaBoost algorithm in the context of fairness. We use the Census Income Dataset (Lichman, 2013) as a case study. We empirically evaluate the bias and error of four variants of AdaBoost relative to an unmodified AdaBoost baseline, and study the trade-offs between reducing bias and maintaining low error. We further define a new notion of fairness and measure it for all of our methods. Our proposed method, modifying the hypothesis output by AdaBoost by shifting the decision boundary for the protected group, outperforms the state of the art for the census dataset.</p>
    
</div>	  

<div class="container" style="font-size:14pt">
    <h2><a name="jelveh"></a>Zubin Jelveh and Michael Luca</h2>
	<h3><a href="papers/Jelveh_Luca.pdf">Towards Diagnosing Accuracy Loss in Discrimination-Aware Classification: An Application to Predictive Policing</a></h3>

    <p>Prediction algorithms are increasingly used to forecast outcomes of processes that are societally sensitive. In response, algorithms have been developed to produce fair classifications but at the potential cost of accuracy. In this work, we present a framework for modeling the pathways by which sensitive variables influence—and are influenced by—nonsensitive variables. These pathways allow us to discern between two types of accuracy loss: justified reduction due to underlying discrimination in the data, and overadjustment due to the removal of nonsensitive predictive information. We also present a framework for adjusting input data to remove the association between sensitive and nonsensitive predictors and assess its ability to produce fair classifications. Finally, we apply our methodology to a new dataset in the criminal justice domain.</p>
    
</div>	  
	  
<div class="container" style="font-size:14pt">
    <h2><a name="zliobaite"></a>Indrė Žliobaitė</h2>
	<h3><a href="papers/Žliobaitė.pdf">On the Relation between Accuracy and Fairness in Binary Classification</a></h3>
	
    <p>Our study revisits the problem of accuracy/fairness tradeoff in binary classification. We argue that comparison of non-discriminatory classifiers needs to account for different rates of positive predictions, otherwise conclusions about performance may be misleading, because accuracy and discrimination of naive baselines on the same dataset vary with different rates of positive predictions. We provide methodological recommendations for sound comparison of nondiscriminatory classifiers, and present a brief theoretical and empirical analysis of tradeoffs between accuracy and non-discrimination.</p>


<h1 class="featurette-heading extra-space">Call for Papers</h1>


<pre class="container" style="font-size:14pt">
CALL FOR PAPERS
==============================================================================

2nd Workshop on Fairness, Accountability, and Transparency in Machine Learning

ICML 2015

July 11, Lille, France

http://fatml.org/

Submission Deadline: May 1, 2015

==============================================================================

OVERVIEW
--------

Machine learning is increasingly part of our everyday lives, influencing not
only our individual interactions with online websites and platforms, but even
national policy decisions that shape society at large. When algorithms make
automated decisions that can affect our lives so profoundly, how do we make
sure that their decisions are fair, verifiable, and accountable? This workshop
will explore how to integrate these concerns into machine learning and how to
address them with computationally rigorous methods.

The workshop takes place at an important moment. The debate about ‘big data'
on both sides of the Atlantic has begun to expand beyond issues of privacy and
data protection. Policymakers, regulators, and advocates have recently
expressed fears about the potentially discriminatory impact of analytics, with
many calling for further technical research into the dangers of inadvertently
encoding bias into automated decisions.  At the same time, there is growing
alarm that the complexity of machine learning may reduce the justification for
consequential decisions to “the algorithm made me do it”.  Decision procedures
perceived as fundamentally inscrutable have drawn special attention.

The workshop will bring together an interdisciplinary group of researchers to
address these challenges head-on.

TOPICS OF INTEREST
------------------

We welcome contributions on theoretical models, empirical work, and everything
in between, including (but not limited to) contributions that address the
following open questions:

* How can we achieve high classification accuracy while preventing
discriminatory biases?

* What are meaningful formal fairness properties?

* What is the best way to represent how a classifier or model has generated a
particular result?

* Can we certify that some output has an explanatory representation?

* How do we balance the need for knowledge of sensitive attributes for  fair
modeling and classification with concerns and limitations around the
collection and use of sensitive attributes?

* What ethical obligations does the machine learning community have when
models affect the lives of real people?

PAPER SUBMISSION
----------------

Papers are limited to four content pages, including figures and tables, and
must follow the ICML 2015 format; however, an additional fifth page containing
only cited references is permitted. Papers SHOULD be anonymized. Accepted
papers will be made available on the workshop website; however, the workshop's
proceedings can be considered non-archival, meaning contributors are free to
publish their work in archival journals or conferences. Accepted papers will
be either presented as a talk or poster (to be determined by the workshop
organizers).

Papers should be submitted here:
https://easychair.org/conferences/?conf=fatml2015

Deadline for submissions: May 1, 2015 Notification of acceptance: May 10, 2015

ORGANIZATION
------------

Workshop Organizers:

Solon Barocas, Princeton University
Sorelle Friedler, Haverford College
Moritz Hardt, IBM Almaden Research Center
Joshua Kroll, Princeton University
Carlos Scheidegger, University of Arizona
Suresh Venkatasubramanian, University of Utah
Hanna Wallach, Microsoft Research NYC
</pre>



    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
  </body>
</html>

